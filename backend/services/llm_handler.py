import requests
import json
import re


class LLMHandler:
    def __init__(self):
        
        self.ollama_url = "http://127.0.0.1:11434/api/generate"
        self.chat_model = "qwen2.5:1.5b"
        self.feedback_model = "qwen2.5:3b"
        self.hsk_level = "5-6" # tune text highlighting for which hsk levels

    
    def get_response(self, user_message, conversation_history):
        """Get user llm chat response"""
        try:
            context = self._build_context(conversation_history)

            system_prompt = (
                f"<system>\n"
                f"ä½ æ˜¯ç”¨æˆ·çš„ä¸­æ–‡æœ‹å‹ï¼Œç”¨è‡ªç„¶ã€è½»æ¾çš„æ–¹å¼èŠå¤©ã€‚"
                f"åƒçœŸæ­£çš„æœ‹å‹ä¸€æ ·äº¤æµâ€”â€”åˆ†äº«æƒ³æ³•ã€å›åº”æ„Ÿå—ã€å»¶ç»­è¯é¢˜ã€‚\n\n"
                f"å¯¹è¯é£æ ¼ï¼š\n"
                f"- ç”¨æ—¥å¸¸å£è¯­ï¼ˆHSK{self.hsk_level}éš¾åº¦ï¼‰ï¼Œåƒæœ‹å‹èŠå¤©ä¸€æ ·è‡ªç„¶\n"
                f"- å›åº”ç”¨æˆ·è¯´çš„å†…å®¹ï¼Œè¡¨è¾¾çœŸå®çš„ååº”å’Œæƒ³æ³•\n"
                f"- æé—®è¦å‡ºäºå¥½å¥‡ï¼Œä¸æ˜¯ä¸ºäº†æ•™å­¦â€”â€”å°±åƒæœ‹å‹æƒ³äº†è§£æ›´å¤š\n"
                f"- å¶å°”åˆ†äº«ä½ çš„'æƒ³æ³•'æˆ–'ç»å†'è®©å¯¹è¯æ›´çœŸå®\n"
                f"- ä¿æŒç®€çŸ­è‡ªç„¶ï¼Œä¸€ä¸¤å¥è¯å°±å¤Ÿäº†\n\n"
                f"å¤„ç†ç”¨æˆ·çš„ä¸­æ–‡ï¼š\n"
                f"- æ°¸è¿œä¸è¦æ˜ç¡®çº é”™æˆ–æ‰“æ–­å¯¹è¯æµ\n"
                f"- å¦‚æœç”¨æˆ·çš„è¡¨è¾¾å¾ˆä¸è‡ªç„¶æˆ–å¾ˆå¥‡æ€ªï¼Œä½ å¯ä»¥åœ¨å›åº”ä¸­è‡ªç„¶åœ°ç”¨æ­£ç¡®è¯´æ³•é‡å¤é‚£ä¸ªæ„æ€\n"
                f"  ä¾‹å¦‚ï¼šç”¨æˆ·è¯´'æˆ‘æ˜¨å¤©å»äº†çœ‹ç”µå½±'ï¼Œä½ è¯´'å“¦çœŸçš„å—ï¼Ÿä½ æ˜¨å¤©çœ‹äº†ä»€ä¹ˆç”µå½±ï¼Ÿ'\n"
                f"- åªåœ¨è¡¨è¾¾çœŸçš„å¾ˆä¸è‡ªç„¶æ—¶æ‰è¿™æ ·åšï¼Œä¸è¦æ¯æ¬¡éƒ½é‡å¤\n"
                f"- å¦‚æœç”¨æˆ·æ··äº†è‹±æ–‡è¯ï¼Œå¯ä»¥è‡ªç„¶åœ°ç”¨ä¸­æ–‡è¯´é‚£ä¸ªè¯ï¼Œä½†è¦åƒæœ‹å‹æé†’ï¼Œä¸æ˜¯è€å¸ˆçº æ­£\n\n"
                f"è®°ä½ï¼šä½ ä¸æ˜¯è€å¸ˆæˆ–é¢è¯•å®˜ï¼Œä½ æ˜¯æœ‹å‹ã€‚è®©å¯¹è¯è‡ªç„¶æµåŠ¨ã€‚"
                f"</system>"
            )

            prompt = (
                f"{system_prompt}\n"
                f"<conversation>\n"
                f"{context}\n"
                f"</conversation>\n"  
                f"<user>{user_message}</user>\n"
                f"<assistant>"
            )

            response = self._call_ollama(self.chat_model, prompt, 0.9, 0.9, keep_alive="5m")

            if response.status_code != 200:
                return "æŠ±æ­‰ï¼Œæˆ‘å¥½åƒé‡åˆ°ä¸€ç‚¹é—®é¢˜ï¼Œä½ å¯ä»¥å†è¯´ä¸€æ¬¡å—ï¼Ÿ"

            result_text = self._safe_parse_response(response).strip()

            if not self._looks_like_mandarin(result_text):
                result_text = "æˆ‘å†è¯´ä¸€éï¼š" + result_text

            return result_text

        except Exception as e:
            print(f"LLM Error: {e}")
            return "ç³»ç»Ÿå¥½åƒè¿ä¸ä¸Šï¼Œä½ å¯ä»¥å†è¯•ä¸€æ¬¡å—ï¼Ÿ"

    # user feedback model to correct sentences
    def correct_sentence(self, broken_sentence):
        """
        NEW: Natural correction with smart highlighting.
        
        Returns: {
            "corrected": "è‡ªç„¶çš„ä¸­æ–‡å¥å­",
            "highlights": [
                {
                    "word": "ä¸­æ–‡è¯",
                    "meaning": "English meaning",
                    "why": "ä¸ºä»€ä¹ˆå€¼å¾—å­¦ä¹ ",
                    "category": "new_vocab|measure_word|collocation|idiom"
                }
            ],
            "note": "æ•´ä½“è¯„ä»·ï¼ˆå¯é€‰ï¼‰"
        }
        """
        
        # Extract English words to hint to the LLM
        english_words = re.findall(r'[a-zA-Z]+', broken_sentence)
        english_hint = f"\næ³¨æ„ï¼šç”¨æˆ·ç”¨è‹±æ–‡è¯´äº†è¿™äº›è¯ï¼š{', '.join(english_words)}" if english_words else ""
        
        # Build the new prompt
        prompt = f"""ä½ æ˜¯ç»éªŒä¸°å¯Œçš„ä¸­æ–‡è€å¸ˆã€‚å­¦ç”Ÿï¼ˆä¸­çº§æ°´å¹³ï¼ŒHSK 3-4ï¼‰è¯´äº†ï¼š

"{broken_sentence}"{english_hint}

ä»»åŠ¡ï¼š
1. æ”¹æˆæ¯è¯­è€…ä¼šè¯´çš„è‡ªç„¶å¥å­
2. æ ‡æ³¨å€¼å¾—å­¦ä¹ çš„è¯æ±‡ï¼ˆæ–°è¯ã€æ­é…ã€é‡è¯ã€ä¹ è¯­ç­‰ï¼‰

è§„åˆ™ï¼š
- ä¸è¦é€å­—ç¿»è¯‘ï¼è¦è¯´æ¯è¯­è€…çœŸæ­£ä¼šè¯´çš„è¯
- ä»”ç»†ç†è§£å¥å­ç»“æ„å’Œè¯­å¢ƒï¼ˆæ³¨æ„ä¸€è¯å¤šä¹‰ã€åŒå½¢å¼‚ä¹‰è¯ï¼‰
- ä¹ è¯­ã€é—®å€™è¯­è¦ç”¨åœ°é“è¡¨è¾¾ï¼ˆä¾‹å¦‚ï¼š"hello people" â†’ "å¤§å®¶å¥½"ï¼Œä¸æ˜¯"ä½ å¥½äººä»¬"ï¼‰
- é«˜äº®æ‰€æœ‰å€¼å¾—æ³¨æ„çš„è¯æ±‡ï¼š
  * ç”¨æˆ·ç”¨è‹±æ–‡è¯´çš„è¯ï¼ˆè¯´æ˜ä»–ä»¬ä¸çŸ¥é“ä¸­æ–‡æ€ä¹ˆè¯´ï¼‰
  * ä¸­é«˜çº§è¯æ±‡ï¼ˆHSK 3+ï¼‰
  * é‡è¯ã€æ­é…ã€ä¹ è¯­
  * ä¸è¦æ ‡æ³¨"æˆ‘"ã€"çš„"ã€"æ˜¯"è¿™ç§æœ€åŸºç¡€çš„è¯
- é‡è¯å¦‚æœç”¨é”™ï¼Œè¦æ ‡æ³¨æ­£ç¡®çš„é‡è¯
- å¦‚æœå¥å­å·²ç»å¾ˆè‡ªç„¶ï¼Œcorrected å¯ä»¥è·ŸåŸå¥ç›¸åŒ

è¾“å‡ºJSONï¼ˆæ— å…¶ä»–æ–‡å­—ï¼‰ï¼š
{{
  "corrected": "è‡ªç„¶æµç•…çš„ä¸­æ–‡å¥å­",
  "highlights": [
    {{
      "word": "é«˜äº®çš„ä¸­æ–‡è¯",
      "meaning": "è‹±æ–‡å«ä¹‰",
      "why": "ä¸ºä»€ä¹ˆå€¼å¾—å­¦ä¹ ï¼ˆä¾‹å¦‚ï¼šå¸¸ç”¨æ­é…ã€æ­£å¼ç”¨è¯­ã€é‡è¯ç­‰ï¼‰",
      "category": "new_vocab|collocation|measure_word|idiom"
    }}
  ],
  "note": "æ•´ä½“è¯„ä»·ï¼ˆå¯é€‰ï¼Œå¦‚æœå¥å­æ”¹åŠ¨è¾ƒå¤§æ‰å†™ï¼‰"
}}

ç¤ºä¾‹1 - æ··åˆä¸­è‹±æ–‡ï¼š
è¾“å…¥: "æˆ‘æƒ³bookä¸€ä¸ªrestaurantï¼Œä½ æœ‰recommendationå—ï¼Ÿ"
è¾“å‡º: {{
  "corrected": "æˆ‘æƒ³é¢„è®¢ä¸€å®¶é¤å…ï¼Œä½ æœ‰æ¨èçš„å—ï¼Ÿ",
  "highlights": [
    {{"word": "é¢„è®¢", "meaning": "to reserve/book", "why": "æ­£å¼åœºåˆç”¨è¯", "category": "new_vocab"}},
    {{"word": "ä¸€å®¶é¤å…", "meaning": "a restaurant", "why": "é¤å…çš„é‡è¯æ˜¯'å®¶'", "category": "measure_word"}},
    {{"word": "æ¨è", "meaning": "recommend", "why": "å¸¸ç”¨åŠ¨è¯", "category": "new_vocab"}}
  ],
  "note": ""
}}

ç¤ºä¾‹2 - è¯­æ³•/è¯­åºé”™è¯¯ï¼š
è¾“å…¥: "æˆ‘æ˜¨å¤©å»äº†å•†åº—å¾ˆå¤šäºº"
è¾“å‡º: {{
  "corrected": "æˆ‘æ˜¨å¤©å»äº†å•†åº—ï¼Œäººå¾ˆå¤š",
  "highlights": [
    {{"word": "äººå¾ˆå¤š", "meaning": "very crowded (lit: people very many)", "why": "æ­£ç¡®è¯­åºï¼šä¸»è¯­+å¾ˆ+å½¢å®¹è¯", "category": "collocation"}}
  ],
  "note": "éœ€è¦ç”¨é€—å·åˆ†éš”ä¸¤ä¸ªä¿¡æ¯"
}}

ç¤ºä¾‹3 - å·²ç»å¾ˆè‡ªç„¶ï¼š
è¾“å…¥: "æˆ‘ä»Šå¤©å¾ˆç´¯ï¼Œæƒ³æ—©ç‚¹ç¡è§‰"
è¾“å‡º: {{
  "corrected": "æˆ‘ä»Šå¤©å¾ˆç´¯ï¼Œæƒ³æ—©ç‚¹ç¡è§‰",
  "highlights": [],
  "note": "è¯´å¾—å¾ˆè‡ªç„¶ï¼"
}}

ç¤ºä¾‹4 - çº¯è‹±æ–‡ï¼ˆä¹ è¯­è¡¨è¾¾ï¼‰ï¼š
è¾“å…¥: "hello people"
è¾“å‡º: {{
  "corrected": "å¤§å®¶å¥½",
  "highlights": [
    {{"word": "å¤§å®¶å¥½", "meaning": "hello everyone (idiomatic greeting)", "why": "ä¹ æƒ¯ç”¨è¯­ï¼Œæ¯”'ä½ å¥½äººä»¬'æ›´è‡ªç„¶", "category": "idiom"}}
  ],
  "note": ""
}}

ç¤ºä¾‹5 - çº¯è‹±æ–‡ï¼ˆç›´æ¥ç¿»è¯‘ï¼‰ï¼š
è¾“å…¥: "I want to learn Chinese"
è¾“å‡º: {{
  "corrected": "æˆ‘æƒ³å­¦ä¸­æ–‡",
  "highlights": [
    {{"word": "å­¦", "meaning": "to learn/study", "why": "åŸºç¡€åŠ¨è¯", "category": "new_vocab"}},
    {{"word": "ä¸­æ–‡", "meaning": "Chinese language", "why": "ä¸'æ±‰è¯­'åŒä¹‰ï¼Œæ›´å£è¯­åŒ–", "category": "new_vocab"}}
  ],
  "note": ""
}}

ç¤ºä¾‹6 - çº¯è‹±æ–‡ï¼ˆæ³¨æ„ä¸€è¯å¤šä¹‰ï¼‰ï¼š
è¾“å…¥: "I hit a bat with a bat"
è¾“å‡º: {{
  "corrected": "æˆ‘ç”¨çƒæ£’æ‰“äº†ä¸€åªè™è ",
  "highlights": [
    {{"word": "çƒæ£’", "meaning": "baseball bat (sports equipment)", "why": "è¿åŠ¨å™¨æ", "category": "new_vocab"}},
    {{"word": "è™è ", "meaning": "bat (animal)", "why": "åŠ¨ç‰©åç§°", "category": "new_vocab"}},
    {{"word": "ç”¨...æ‰“", "meaning": "to hit with (using...)", "why": "å¸¸ç”¨ç»“æ„", "category": "collocation"}}
  ],
  "note": "æ³¨æ„ï¼šbat æœ‰ä¸¤ä¸ªæ„æ€ï¼ˆçƒæ£’/è™è ï¼‰"
}}

ç°åœ¨å¤„ç†ï¼š
"{broken_sentence}"

è¾“å‡ºJSONï¼š"""

        try:
            # Unload chat model, load feedback model
            requests.post(self.ollama_url, json={"model": self.chat_model, "keep_alive": 0})
            print("â†’ Loading 3B Feedback Model...")
            
            response = self._call_ollama(self.feedback_model, prompt, 0.3, 0.9, 0)
            raw = self._safe_parse_response(response).strip()
            
            # Clean and parse JSON
            parsed = self._parse_json_or_fallback(raw)
            
            # Validate structure
            if not parsed.get('corrected'):
                parsed['corrected'] = broken_sentence
            if 'highlights' not in parsed:
                parsed['highlights'] = []
            if 'note' not in parsed:
                parsed['note'] = ''
            
            print(f"âœ… Corrected: {parsed['corrected']}")
            print(f"ğŸ“š Highlights: {len(parsed['highlights'])} items")
            
            return parsed
            
        except Exception as e:
            print(f"[ERROR] Correction failed: {e}")
            return {
                "corrected": broken_sentence,
                "highlights": [],
                "note": f"Error: {str(e)}"
            }

    # helper functions
    def _build_context(self, history):
        """Converts conversation history into XML-tag structure."""
        formatted = []
        for msg in history[-8:]:
            if msg["role"] == "user":
                formatted.append(f"<user>{msg['content']}</user>")
            else:
                formatted.append(f"<assistant>{msg['content']}</assistant>")
        return "\n".join(formatted)

    def _safe_parse_response(self, response):
        """ensure response from llm is valid and usable"""
        try:
            data = response.json()
            return data.get("response", "")
        except Exception as e:
            print(f"Parsing error: {e}")
            return ""

    def _looks_like_mandarin(self, text):
        """Check if output contains Chinese characters."""
        return bool(re.search(r"[\u4e00-\u9fff]", text))

    def _parse_json_or_fallback(self, text):
        """Try to parse JSON from model output, with fallback."""
        text = text.strip()
        text = text.replace("```json", "").replace("```", "")
        
        try:
            return json.loads(text)
        except:
            match = re.search(r'\{.*\}', text, re.DOTALL)
            if match:
                try:
                    return json.loads(match.group(0))
                except:
                    pass
            return {
                "corrected": text,
                "highlights": [],
                "note": "è§£æé”™è¯¯"
            }
            
    def _call_ollama(self, model, prompt, temperature=0.2, top_p=0.9, keep_alive="5m"):
        """Unified caller to handle VRAM memory swap."""
        payload = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            "keep_alive": keep_alive,
            "options": {
                "num_ctx": 2048,
                "temperature": temperature,
                "top_p": top_p,
            }
        }
        return requests.post(self.ollama_url, json=payload)